{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model selection\n",
    "\n",
    "Initialisation\n",
    "- If reproducibility of results is needed, set the seed of the random number generator as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data onboarding for loading, understanding, transforming and splitting for supervised learning\n",
    "- Load the data\n",
    "- Find out how the data oriented and determine the sizes\n",
    "- Visualise the data\n",
    "- Transform the data set so that it is suitable for training a neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 60\n",
      "Global min: -5.28682646609656\n",
      "Global max: 1969.19770431434\n",
      "Number of classes: 5\n",
      "Samples per class:\n",
      "Class 0: 5850 samples\n",
      "Class 1: 6220 samples\n",
      "Class 2: 5396 samples\n",
      "Class 3: 3956 samples\n",
      "Class 4: 2653 samples\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"humanactivity.csv\")\n",
    "\n",
    "# Creating the data\n",
    "X = df.iloc[:,1:].values\n",
    "\n",
    "num_features = X.shape[1]\n",
    "print(\"Number of features:\", num_features)\n",
    "print(\"Global min:\", X.min())\n",
    "print(\"Global max:\", X.max())\n",
    "\n",
    "\n",
    "y = df.iloc[:,0].values.astype(int) -1\n",
    "\n",
    "num_classes = len(np.unique(y))\n",
    "print(\"Number of classes:\", num_classes)\n",
    "\n",
    "unique, counts = np.unique(y, return_counts=True)\n",
    "class_counts = dict(zip(unique, counts))\n",
    "print(\"Samples per class:\")\n",
    "for cls, cnt in class_counts.items():\n",
    "    print(f\"Class {cls}: {cnt} samples\")\n",
    "\n",
    "# train/test split\n",
    "split = int(0.8 * X.shape[0])\n",
    "X_train, y_train = X[:split], y[:split]\n",
    "X_test, y_test = X[split:], y[split:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural network architecture\n",
    "- Design a neural network for solving the classification task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(x):\n",
    "    e = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return e / e.sum(axis=1, keepdims=True)\n",
    "\n",
    "def cross_entropy(pred, target):\n",
    "    # target: one-hot\n",
    "    return -np.mean(np.sum(target * np.log(pred + 1e-12), axis=1))\n",
    "\n",
    "class RNNClassifier:\n",
    "    def __init__(self, input_size, hidden_size, output_size, lr=1e-3, seed=0):\n",
    "        rng = np.random.RandomState(seed)\n",
    "\n",
    "        self.Wxh = rng.randn(hidden_size, input_size) / np.sqrt(input_size)\n",
    "        self.Whh = rng.randn(hidden_size, hidden_size) / np.sqrt(hidden_size)\n",
    "        self.Why = rng.randn(output_size, hidden_size) / np.sqrt(hidden_size)\n",
    "\n",
    "        self.bh = np.zeros((hidden_size, 1))\n",
    "        self.by = np.zeros((output_size, 1))\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lr = lr\n",
    "\n",
    "    def forward(self, X):\n",
    "        batch, seq_len, _ = X.shape\n",
    "        h = np.zeros((batch, seq_len + 1, self.hidden_size))\n",
    "\n",
    "        for t in range(seq_len):\n",
    "            xt = X[:, t, :]\n",
    "            pre = xt @ self.Wxh.T + h[:, t, :] @ self.Whh.T + self.bh.T\n",
    "            h[:, t+1, :] = np.tanh(pre)\n",
    "\n",
    "        logits = h[:, -1, :] @ self.Why.T + self.by.T\n",
    "        y_pred = softmax(logits)\n",
    "\n",
    "        return h, y_pred\n",
    "\n",
    "    def backward(self, X, h, y_pred, y_true):\n",
    "        batch, seq_len, _ = X.shape\n",
    "\n",
    "        dWxh = np.zeros_like(self.Wxh)\n",
    "        dWhh = np.zeros_like(self.Whh)\n",
    "        dWhy = np.zeros_like(self.Why)\n",
    "        dbh = np.zeros_like(self.bh)\n",
    "        dby = np.zeros_like(self.by)\n",
    "\n",
    "        dy = (y_pred - y_true) / batch\n",
    "        h_last = h[:, -1, :]\n",
    "\n",
    "        dWhy += dy.T @ h_last\n",
    "        dby += dy.sum(axis=0, keepdims=True).T\n",
    "\n",
    "        dh_next = dy @ self.Why\n",
    "\n",
    "        for t in reversed(range(seq_len)):\n",
    "            ht = h[:, t+1, :]\n",
    "            ht_prev = h[:, t, :]\n",
    "\n",
    "            dt = dh_next * (1 - ht**2)  # tanh'\n",
    "\n",
    "            dbh += dt.sum(axis=0)[:, None]\n",
    "            dWxh += dt.T @ X[:, t, :]\n",
    "            dWhh += dt.T @ ht_prev\n",
    "\n",
    "            dh_next = dt @ self.Whh\n",
    "\n",
    "        # clip\n",
    "        for g in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "            np.clip(g, -5, 5, out=g)\n",
    "\n",
    "        # update\n",
    "        self.Wxh -= self.lr * dWxh\n",
    "        self.Whh -= self.lr * dWhh\n",
    "        self.Why -= self.lr * dWhy\n",
    "        self.bh  -= self.lr * dbh\n",
    "        self.by  -= self.lr * dby\n",
    "\n",
    "    def train(self, X, y, epochs=100, batch_size=16):\n",
    "        loss_history = []\n",
    "        n = X.shape[0]\n",
    "        for epoch in range(epochs):\n",
    "            perm = np.random.permutation(n)\n",
    "            epoch_loss = 0\n",
    "\n",
    "            for i in range(0, n, batch_size):\n",
    "                xb = X[perm[i:i+batch_size]]\n",
    "                yb = y[perm[i:i+batch_size]]\n",
    "\n",
    "                h, y_pred = self.forward(xb)\n",
    "                loss = cross_entropy(y_pred, yb)\n",
    "                epoch_loss += loss * xb.shape[0]\n",
    "\n",
    "                self.backward(xb, h, y_pred, yb)\n",
    "            \n",
    "            epoch_loss /= n\n",
    "            loss_history.append(epoch_loss)\n",
    "\n",
    "            print(f\"Epoch {epoch+1}: loss={epoch_loss:.4f}\")\n",
    "        return loss_history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model training\n",
    "- Train the selected model using the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: loss=0.2242\n",
      "Epoch 2: loss=0.1659\n",
      "Epoch 3: loss=0.1579\n",
      "Epoch 4: loss=0.1554\n",
      "Epoch 5: loss=0.1502\n",
      "Epoch 6: loss=0.1531\n",
      "Epoch 7: loss=0.1421\n",
      "Epoch 8: loss=0.1384\n",
      "Epoch 9: loss=0.1434\n",
      "Epoch 10: loss=0.1415\n",
      "Epoch 11: loss=0.1422\n",
      "Epoch 12: loss=0.1368\n",
      "Epoch 13: loss=0.1476\n",
      "Epoch 14: loss=0.1380\n",
      "Epoch 15: loss=0.1334\n",
      "Epoch 16: loss=0.1340\n",
      "Epoch 17: loss=0.1347\n",
      "Epoch 18: loss=0.1321\n",
      "Epoch 19: loss=0.1244\n",
      "Epoch 20: loss=0.1316\n",
      "Epoch 21: loss=0.1276\n",
      "Epoch 22: loss=0.1302\n",
      "Epoch 23: loss=0.1287\n",
      "Epoch 24: loss=0.1250\n",
      "Epoch 25: loss=0.1270\n",
      "Epoch 26: loss=0.1306\n",
      "Epoch 27: loss=0.1306\n",
      "Epoch 28: loss=0.1310\n",
      "Epoch 29: loss=0.1299\n",
      "Epoch 30: loss=0.1299\n",
      "Epoch 31: loss=0.1207\n",
      "Epoch 32: loss=0.1172\n",
      "Epoch 33: loss=0.1261\n",
      "Epoch 34: loss=0.1224\n",
      "Epoch 35: loss=0.1205\n",
      "Epoch 36: loss=0.1195\n",
      "Epoch 37: loss=0.1201\n",
      "Epoch 38: loss=0.1215\n",
      "Epoch 39: loss=0.1225\n",
      "Epoch 40: loss=0.1200\n",
      "Epoch 41: loss=0.1191\n",
      "Epoch 42: loss=0.1211\n",
      "Epoch 43: loss=0.1184\n",
      "Epoch 44: loss=0.1236\n",
      "Epoch 45: loss=0.1199\n",
      "Epoch 46: loss=0.1208\n",
      "Epoch 47: loss=0.1197\n",
      "Epoch 48: loss=0.1169\n",
      "Epoch 49: loss=0.1194\n",
      "Epoch 50: loss=0.1205\n",
      "Epoch 51: loss=0.1181\n",
      "Epoch 52: loss=0.1189\n",
      "Epoch 53: loss=0.1241\n",
      "Epoch 54: loss=0.1172\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[59]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m      7\u001b[39m y_test_oh  = np.eye(num_classes)[y_test]\n\u001b[32m      9\u001b[39m rnn = RNNClassifier(input_size=X_train.shape[\u001b[32m2\u001b[39m],\n\u001b[32m     10\u001b[39m                     hidden_size=\u001b[32m256\u001b[39m,\n\u001b[32m     11\u001b[39m                     output_size=num_classes,\n\u001b[32m     12\u001b[39m                     lr=\u001b[32m1e-2\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m losses = \u001b[43mrnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_oh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m plt.plot(losses)\n\u001b[32m     17\u001b[39m plt.xlabel(\u001b[33m\"\u001b[39m\u001b[33mEpoch\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[58]\u001b[39m\u001b[32m, line 90\u001b[39m, in \u001b[36mRNNClassifier.train\u001b[39m\u001b[34m(self, X, y, epochs, batch_size)\u001b[39m\n\u001b[32m     87\u001b[39m xb = X[perm[i:i+batch_size]]\n\u001b[32m     88\u001b[39m yb = y[perm[i:i+batch_size]]\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m h, y_pred = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     91\u001b[39m loss = cross_entropy(y_pred, yb)\n\u001b[32m     92\u001b[39m epoch_loss += loss * xb.shape[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[58]\u001b[39m\u001b[32m, line 32\u001b[39m, in \u001b[36mRNNClassifier.forward\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m     30\u001b[39m     xt = X[:, t, :]\n\u001b[32m     31\u001b[39m     pre = xt @ \u001b[38;5;28mself\u001b[39m.Wxh.T + h[:, t, :] @ \u001b[38;5;28mself\u001b[39m.Whh.T + \u001b[38;5;28mself\u001b[39m.bh.T\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m     h[:, t+\u001b[32m1\u001b[39m, :] = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtanh\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpre\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m logits = h[:, -\u001b[32m1\u001b[39m, :] @ \u001b[38;5;28mself\u001b[39m.Why.T + \u001b[38;5;28mself\u001b[39m.by.T\n\u001b[32m     35\u001b[39m y_pred = softmax(logits)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# reshape for RNN\n",
    "X_train = X_train.reshape(len(X_train), 1, -1)\n",
    "X_test  = X_test.reshape(len(X_test), 1, -1)\n",
    "\n",
    "num_classes = len(np.unique(y_train))\n",
    "y_train_oh = np.eye(num_classes)[y_train]\n",
    "y_test_oh  = np.eye(num_classes)[y_test]\n",
    "\n",
    "rnn = RNNClassifier(input_size=X_train.shape[2],\n",
    "                    hidden_size=256,\n",
    "                    output_size=num_classes,\n",
    "                    lr=1e-2)\n",
    "\n",
    "losses = rnn.train(X_train, y_train_oh, epochs=100)\n",
    "\n",
    "plt.plot(losses)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss Curve\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model testing\n",
    "- Classify the test set samples by applying the trained/optimised model\n",
    "- Analyse the experimental results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.9607476635514018\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Predict\n",
    "_, y_pred_test = rnn.forward(X_test)\n",
    "\n",
    "# Class label predictions\n",
    "pred_classes = np.argmax(y_pred_test, axis=1)\n",
    "true_classes = np.argmax(y_test_oh, axis=1)\n",
    "\n",
    "accuracy = np.mean(pred_classes == true_classes)\n",
    "print(\"Test accuracy:\", accuracy)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(true_classes, pred_classes)\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.xlabel(\"Predicted Class\")\n",
    "plt.ylabel(\"True Class\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
